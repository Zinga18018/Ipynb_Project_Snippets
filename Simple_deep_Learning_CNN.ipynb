{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh9+aYurGDFKUC1E3X6ar8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinga18018/Ipynb_Project_Snippets/blob/main/Simple_deep_Learning_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "816c4b7a"
      },
      "source": [
        "### Project: Image Classification with CNNs on CIFAR-10 using PyTorch\n",
        "\n",
        "**Goal**: Build a Convolutional Neural Network (CNN) to classify images from the CIFAR-10 dataset.\n",
        "\n",
        "**CIFAR-10 Dataset**: This dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.\n",
        "\n",
        "Here's a breakdown of the steps we'll take:\n",
        "\n",
        "1.  **Load and Prepare Data**: Download and load the CIFAR-10 dataset, apply necessary transformations.\n",
        "2.  **Define the CNN Model**: Design a simple CNN architecture using PyTorch's `nn.Module`.\n",
        "3.  **Define Loss Function and Optimizer**: Choose appropriate loss and optimization techniques.\n",
        "4.  **Train the Model**: Write a training loop to iterate over the dataset, perform forward and backward passes, and update model weights.\n",
        "5.  **Evaluate the Model**: Test the trained model on the unseen test set and report accuracy.\n",
        "6.  **Visualize Results (Optional but Recommended)**: Display some predictions and misclassifications.\n",
        "\n",
        "Let's start by setting up the environment and loading the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac569fb3"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eb2c89a"
      },
      "source": [
        "### Step 1: Load and Prepare Data\n",
        "\n",
        "We'll define transformations for the images (e.g., converting them to tensors and normalizing them). Then, we'll download the CIFAR-10 training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90f02f07"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(\"Data loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d56f10fe"
      },
      "source": [
        "Let's visualize a few training images to understand the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e6e6aaa"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73c3d542"
      },
      "source": [
        "### Step 2: Define the CNN Model\n",
        "\n",
        "Now we'll define our Convolutional Neural Network. We'll use a simple architecture with a few convolutional layers, pooling layers, and fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17fcdd8a"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5) # 3 input channels (RGB), 6 output channels, 5x5 kernel\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # 16 filters, 5x5 feature map size after pooling\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10) # 10 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "net.to(device)\n",
        "print(\"CNN model defined and moved to device!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d5552af"
      },
      "source": [
        "### Step 3: Define Loss Function and Optimizer\n",
        "\n",
        "For classification, we typically use Cross-Entropy Loss, and for optimization, Stochastic Gradient Descent (SGD) is a common choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "220b9702"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "print(\"Loss function and optimizer defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8850ab49"
      },
      "source": [
        "### Step 4: Train the Model\n",
        "\n",
        "This is the core training loop. We will iterate over the training data multiple times (epochs), feed inputs to the network, get predictions, calculate loss, and update weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d8ec0b7"
      },
      "source": [
        "epochs = 5 # You can increase this for better accuracy\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Save the trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "print(f\"Model saved to {PATH}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7a937f7"
      },
      "source": [
        "### Step 5: Evaluate the Model\n",
        "\n",
        "After training, we'll load the saved model and evaluate its performance on the test dataset to see how well it generalizes to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e56908fc"
      },
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "net.to(device)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "# Evaluate accuracy for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccdb51ed"
      },
      "source": [
        "### Step 6: Visualize Results (Optional)\n",
        "\n",
        "Let's see some predictions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4e3684f"
      },
      "source": [
        "# Get some random test images\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
        "\n",
        "# Make predictions\n",
        "outputs = net(images.to(device))\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted:   ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}